{
  "file_path": "c:\\Users\\sgbil\\OneDrive\\Desktop\\Negative_Space_Imaging_Project\\.venv\\Lib\\site-packages\\imageio\\plugins\\pyav.py",
  "classes": [
    {
      "name": "PyAVPlugin",
      "docstring": "Support for pyAV as backend.\n\nParameters\n----------\nrequest : iio.Request\n    A request object that represents the users intent. It provides a\n    standard interface to access various the various ImageResources and\n    serves them to the plugin as a file object (or file). Check the docs for\n    details.\ncontainer : str\n    Only used during `iio_mode=\"w\"`! If not None, overwrite the default container\n    format chosen by pyav.\nkwargs : Any\n    Additional kwargs are forwarded to PyAV's constructor.",
      "methods": [
        {
          "name": "__init__",
          "docstring": "Initialize a new Plugin Instance.\n\nSee Plugin's docstring for detailed documentation.\n\nNotes\n-----\nThe implementation here stores the request as a local variable that is\nexposed using a @property below. If you inherit from PluginV3, remember\nto call ``super().__init__(request)``."
        },
        {
          "name": "read",
          "docstring": "Read frames from the video.\n\nIf ``index`` is an integer, this function reads the index-th frame from\nthe file. If ``index`` is ... (Ellipsis), this function reads all frames\nfrom the video, stacks them along the first dimension, and returns a\nbatch of frames.\n\nParameters\n----------\nindex : int\n    The index of the frame to read, e.g. ``index=5`` reads the 5th\n    frame. If ``...``, read all the frames in the video and stack them\n    along a new, prepended, batch dimension.\nformat : str\n    Set the returned colorspace. If not None (default: rgb24), convert\n    the data into the given format before returning it. If ``None``\n    return the data in the encoded format if it can be expressed as a\n    strided array; otherwise raise an Exception.\nfilter_sequence : List[str, str, dict]\n    If not None, apply the given sequence of FFmpeg filters to each\n    ndimage. Check the (module-level) plugin docs for details and\n    examples.\nfilter_graph : (dict, List)\n    If not None, apply the given graph of FFmpeg filters to each\n    ndimage. The graph is given as a tuple of two dicts. The first dict\n    contains a (named) set of nodes, and the second dict contains a set\n    of edges between nodes of the previous dict. Check the (module-level)\n    plugin docs for details and examples.\nconstant_framerate : bool\n    If True assume the video's framerate is constant. This allows for\n    faster seeking inside the file. If False, the video is reset before\n    each read and searched from the beginning. If None (default), this\n    value will be read from the container format.\nthread_count : int\n    How many threads to use when decoding a frame. The default is 0,\n    which will set the number using ffmpeg's default, which is based on\n    the codec, number of available cores, threadding model, and other\n    considerations.\nthread_type : str\n    The threading model to be used. One of\n\n    - `\"SLICE\"`: threads assemble parts of the current frame\n    - `\"FRAME\"`: threads may assemble future frames\n    - None (default): Uses ``\"FRAME\"`` if ``index=...`` and ffmpeg's\n      default otherwise.\n\n\nReturns\n-------\nframe : np.ndarray\n    A numpy array containing loaded frame data.\n\nNotes\n-----\nAccessing random frames repeatedly is costly (O(k), where k is the\naverage distance between two keyframes). You should do so only sparingly\nif possible. In some cases, it can be faster to bulk-read the video (if\nit fits into memory) and to then access the returned ndarray randomly.\n\nThe current implementation may cause problems for b-frames, i.e.,\nbidirectionaly predicted pictures. I lack test videos to write unit\ntests for this case.\n\nReading from an index other than ``...``, i.e. reading a single frame,\ncurrently doesn't support filters that introduce delays."
        },
        {
          "name": "iter",
          "docstring": "Yield frames from the video.\n\nParameters\n----------\nframe : np.ndarray\n    A numpy array containing loaded frame data.\nformat : str\n    Convert the data into the given format before returning it. If None,\n    return the data in the encoded format if it can be expressed as a\n    strided array; otherwise raise an Exception.\nfilter_sequence : List[str, str, dict]\n    Set the returned colorspace. If not None (default: rgb24), convert\n    the data into the given format before returning it. If ``None``\n    return the data in the encoded format if it can be expressed as a\n    strided array; otherwise raise an Exception.\nfilter_graph : (dict, List)\n    If not None, apply the given graph of FFmpeg filters to each\n    ndimage. The graph is given as a tuple of two dicts. The first dict\n    contains a (named) set of nodes, and the second dict contains a set\n    of edges between nodes of the previous dict. Check the (module-level)\n    plugin docs for details and examples.\nthread_count : int\n    How many threads to use when decoding a frame. The default is 0,\n    which will set the number using ffmpeg's default, which is based on\n    the codec, number of available cores, threadding model, and other\n    considerations.\nthread_type : str\n    The threading model to be used. One of\n\n    - `\"SLICE\"` (default): threads assemble parts of the current frame\n    - `\"FRAME\"`: threads may assemble future frames (faster for bulk reading)\n\n\nYields\n------\nframe : np.ndarray\n    A (decoded) video frame."
        },
        {
          "name": "write",
          "docstring": "Save a ndimage as a video.\n\nGiven a batch of frames (stacked along the first axis) or a list of\nframes, encode them and add the result to the ImageResource.\n\nParameters\n----------\nndimage : ArrayLike, List[ArrayLike]\n    The ndimage to encode and write to the ImageResource.\ncodec : str\n    The codec to use when encoding frames. Only needed on first write\n    and ignored on subsequent writes.\nis_batch : bool\n    If True (default), the ndimage is a batch of images, otherwise it is\n    a single image. This parameter has no effect on lists of ndimages.\nfps : str\n    The resulting videos frames per second.\nin_pixel_format : str\n    The pixel format of the incoming ndarray. Defaults to \"rgb24\" and can\n    be any stridable pix_fmt supported by FFmpeg.\nout_pixel_format : str\n    The pixel format to use while encoding frames. If None (default)\n    use the codec's default.\nfilter_sequence : List[str, str, dict]\n    If not None, apply the given sequence of FFmpeg filters to each\n    ndimage. Check the (module-level) plugin docs for details and\n    examples.\nfilter_graph : (dict, List)\n    If not None, apply the given graph of FFmpeg filters to each\n    ndimage. The graph is given as a tuple of two dicts. The first dict\n    contains a (named) set of nodes, and the second dict contains a set\n    of edges between nodes of the previous dict. Check the (module-level)\n    plugin docs for details and examples.\n\nReturns\n-------\nencoded_image : bytes or None\n    If the chosen ImageResource is the special target ``\"<bytes>\"`` then\n    write will return a byte string containing the encoded image data.\n    Otherwise, it returns None.\n\nNotes\n-----\nWhen writing ``<bytes>``, the video is finalized immediately after the\nfirst write call and calling write multiple times to append frames is\nnot possible."
        },
        {
          "name": "properties",
          "docstring": "Standardized ndimage metadata.\n\nParameters\n----------\nindex : int\n    The index of the ndimage for which to return properties. If ``...``\n    (Ellipsis, default), return the properties for the resulting batch\n    of frames.\nformat : str\n    If not None (default: rgb24), convert the data into the given format\n    before returning it. If None return the data in the encoded format\n    if that can be expressed as a strided array; otherwise raise an\n    Exception.\n\nReturns\n-------\nproperties : ImageProperties\n    A dataclass filled with standardized image metadata.\n\nNotes\n-----\nThis function is efficient and won't process any pixel data.\n\nThe provided metadata does not include modifications by any filters\n(through ``filter_sequence`` or ``filter_graph``)."
        },
        {
          "name": "metadata",
          "docstring": "Format-specific metadata.\n\nReturns a dictionary filled with metadata that is either stored in the\ncontainer, the video stream, or the frame's side-data.\n\nParameters\n----------\nindex : int\n    If ... (Ellipsis, default) return global metadata (the metadata\n    stored in the container and video stream). If not ..., return the\n    side data stored in the frame at the given index.\nexclude_applied : bool\n    Currently, this parameter has no effect. It exists for compliance with\n    the ImageIO v3 API.\nconstant_framerate : bool\n    If True assume the video's framerate is constant. This allows for\n    faster seeking inside the file. If False, the video is reset before\n    each read and searched from the beginning. If None (default), this\n    value will be read from the container format.\n\nReturns\n-------\nmetadata : dict\n    A dictionary filled with format-specific metadata fields and their\n    values."
        },
        {
          "name": "close",
          "docstring": "Close the Video."
        },
        {
          "name": "init_video_stream",
          "docstring": "Initialize a new video stream.\n\nThis function adds a new video stream to the ImageResource using the\nselected encoder (codec), framerate, and colorspace.\n\nParameters\n----------\ncodec : str\n    The codec to use, e.g. ``\"h264\"`` or ``\"vp9\"``.\nfps : float\n    The desired framerate of the video stream (frames per second).\npixel_format : str\n    The pixel format to use while encoding frames. If None (default) use\n    the codec's default.\nmax_keyframe_interval : int\n    The maximum distance between two intra frames (I-frames). Also known\n    as GOP size. If unspecified use the codec's default. Note that not\n    every I-frame is a keyframe; see the notes for details.\nforce_keyframes : bool\n    If True, limit inter frames dependency to frames within the current\n    keyframe interval (GOP), i.e., force every I-frame to be a keyframe.\n    If unspecified, use the codec's default.\n\nNotes\n-----\nYou can usually leave ``max_keyframe_interval`` and ``force_keyframes``\nat their default values, unless you try to generate seek-optimized video\nor have a similar specialist use-case. In this case, ``force_keyframes``\ncontrols the ability to seek to _every_ I-frame, and\n``max_keyframe_interval`` controls how close to a random frame you can\nseek. Low values allow more fine-grained seek at the expense of\nfile-size (and thus I/O performance)."
        },
        {
          "name": "write_frame",
          "docstring": "Add a frame to the video stream.\n\nThis function appends a new frame to the video. It assumes that the\nstream previously has been initialized. I.e., ``init_video_stream`` has\nto be called before calling this function for the write to succeed.\n\nParameters\n----------\nframe : np.ndarray\n    The image to be appended/written to the video stream.\npixel_format : str\n    The colorspace (pixel format) of the incoming frame.\n\nNotes\n-----\nFrames may be held in a buffer, e.g., by the filter pipeline used during\nwriting or by FFMPEG to batch them prior to encoding. Make sure to\n``.close()`` the plugin or to use a context manager to ensure that all\nframes are written to the ImageResource."
        },
        {
          "name": "set_video_filter",
          "docstring": "Set the filter(s) to use.\n\nThis function creates a new FFMPEG filter graph to use when reading or\nwriting video. In the case of reading, frames are passed through the\nfilter graph before begin returned and, in case of writing, frames are\npassed through the filter before being written to the video.\n\nParameters\n----------\nfilter_sequence : List[str, str, dict]\n    If not None, apply the given sequence of FFmpeg filters to each\n    ndimage. Check the (module-level) plugin docs for details and\n    examples.\nfilter_graph : (dict, List)\n    If not None, apply the given graph of FFmpeg filters to each\n    ndimage. The graph is given as a tuple of two dicts. The first dict\n    contains a (named) set of nodes, and the second dict contains a set\n    of edges between nodes of the previous dict. Check the\n    (module-level) plugin docs for details and examples.\n\nNotes\n-----\nChanging a filter graph with lag during reading or writing will\ncurrently cause frames in the filter queue to be lost."
        },
        {
          "name": "container_metadata",
          "docstring": "Container-specific metadata.\n\nA dictionary containing metadata stored at the container level."
        },
        {
          "name": "video_stream_metadata",
          "docstring": "Stream-specific metadata.\n\nA dictionary containing metadata stored at the stream level."
        }
      ]
    }
  ],
  "functions": [
    {
      "name": "read",
      "docstring": "Read frames from the video.\n\nIf ``index`` is an integer, this function reads the index-th frame from\nthe file. If ``index`` is ... (Ellipsis), this function reads all frames\nfrom the video, stacks them along the first dimension, and returns a\nbatch of frames.\n\nParameters\n----------\nindex : int\n    The index of the frame to read, e.g. ``index=5`` reads the 5th\n    frame. If ``...``, read all the frames in the video and stack them\n    along a new, prepended, batch dimension.\nformat : str\n    Set the returned colorspace. If not None (default: rgb24), convert\n    the data into the given format before returning it. If ``None``\n    return the data in the encoded format if it can be expressed as a\n    strided array; otherwise raise an Exception.\nfilter_sequence : List[str, str, dict]\n    If not None, apply the given sequence of FFmpeg filters to each\n    ndimage. Check the (module-level) plugin docs for details and\n    examples.\nfilter_graph : (dict, List)\n    If not None, apply the given graph of FFmpeg filters to each\n    ndimage. The graph is given as a tuple of two dicts. The first dict\n    contains a (named) set of nodes, and the second dict contains a set\n    of edges between nodes of the previous dict. Check the (module-level)\n    plugin docs for details and examples.\nconstant_framerate : bool\n    If True assume the video's framerate is constant. This allows for\n    faster seeking inside the file. If False, the video is reset before\n    each read and searched from the beginning. If None (default), this\n    value will be read from the container format.\nthread_count : int\n    How many threads to use when decoding a frame. The default is 0,\n    which will set the number using ffmpeg's default, which is based on\n    the codec, number of available cores, threadding model, and other\n    considerations.\nthread_type : str\n    The threading model to be used. One of\n\n    - `\"SLICE\"`: threads assemble parts of the current frame\n    - `\"FRAME\"`: threads may assemble future frames\n    - None (default): Uses ``\"FRAME\"`` if ``index=...`` and ffmpeg's\n      default otherwise.\n\n\nReturns\n-------\nframe : np.ndarray\n    A numpy array containing loaded frame data.\n\nNotes\n-----\nAccessing random frames repeatedly is costly (O(k), where k is the\naverage distance between two keyframes). You should do so only sparingly\nif possible. In some cases, it can be faster to bulk-read the video (if\nit fits into memory) and to then access the returned ndarray randomly.\n\nThe current implementation may cause problems for b-frames, i.e.,\nbidirectionaly predicted pictures. I lack test videos to write unit\ntests for this case.\n\nReading from an index other than ``...``, i.e. reading a single frame,\ncurrently doesn't support filters that introduce delays."
    },
    {
      "name": "iter",
      "docstring": "Yield frames from the video.\n\nParameters\n----------\nframe : np.ndarray\n    A numpy array containing loaded frame data.\nformat : str\n    Convert the data into the given format before returning it. If None,\n    return the data in the encoded format if it can be expressed as a\n    strided array; otherwise raise an Exception.\nfilter_sequence : List[str, str, dict]\n    Set the returned colorspace. If not None (default: rgb24), convert\n    the data into the given format before returning it. If ``None``\n    return the data in the encoded format if it can be expressed as a\n    strided array; otherwise raise an Exception.\nfilter_graph : (dict, List)\n    If not None, apply the given graph of FFmpeg filters to each\n    ndimage. The graph is given as a tuple of two dicts. The first dict\n    contains a (named) set of nodes, and the second dict contains a set\n    of edges between nodes of the previous dict. Check the (module-level)\n    plugin docs for details and examples.\nthread_count : int\n    How many threads to use when decoding a frame. The default is 0,\n    which will set the number using ffmpeg's default, which is based on\n    the codec, number of available cores, threadding model, and other\n    considerations.\nthread_type : str\n    The threading model to be used. One of\n\n    - `\"SLICE\"` (default): threads assemble parts of the current frame\n    - `\"FRAME\"`: threads may assemble future frames (faster for bulk reading)\n\n\nYields\n------\nframe : np.ndarray\n    A (decoded) video frame."
    },
    {
      "name": "write",
      "docstring": "Save a ndimage as a video.\n\nGiven a batch of frames (stacked along the first axis) or a list of\nframes, encode them and add the result to the ImageResource.\n\nParameters\n----------\nndimage : ArrayLike, List[ArrayLike]\n    The ndimage to encode and write to the ImageResource.\ncodec : str\n    The codec to use when encoding frames. Only needed on first write\n    and ignored on subsequent writes.\nis_batch : bool\n    If True (default), the ndimage is a batch of images, otherwise it is\n    a single image. This parameter has no effect on lists of ndimages.\nfps : str\n    The resulting videos frames per second.\nin_pixel_format : str\n    The pixel format of the incoming ndarray. Defaults to \"rgb24\" and can\n    be any stridable pix_fmt supported by FFmpeg.\nout_pixel_format : str\n    The pixel format to use while encoding frames. If None (default)\n    use the codec's default.\nfilter_sequence : List[str, str, dict]\n    If not None, apply the given sequence of FFmpeg filters to each\n    ndimage. Check the (module-level) plugin docs for details and\n    examples.\nfilter_graph : (dict, List)\n    If not None, apply the given graph of FFmpeg filters to each\n    ndimage. The graph is given as a tuple of two dicts. The first dict\n    contains a (named) set of nodes, and the second dict contains a set\n    of edges between nodes of the previous dict. Check the (module-level)\n    plugin docs for details and examples.\n\nReturns\n-------\nencoded_image : bytes or None\n    If the chosen ImageResource is the special target ``\"<bytes>\"`` then\n    write will return a byte string containing the encoded image data.\n    Otherwise, it returns None.\n\nNotes\n-----\nWhen writing ``<bytes>``, the video is finalized immediately after the\nfirst write call and calling write multiple times to append frames is\nnot possible."
    },
    {
      "name": "properties",
      "docstring": "Standardized ndimage metadata.\n\nParameters\n----------\nindex : int\n    The index of the ndimage for which to return properties. If ``...``\n    (Ellipsis, default), return the properties for the resulting batch\n    of frames.\nformat : str\n    If not None (default: rgb24), convert the data into the given format\n    before returning it. If None return the data in the encoded format\n    if that can be expressed as a strided array; otherwise raise an\n    Exception.\n\nReturns\n-------\nproperties : ImageProperties\n    A dataclass filled with standardized image metadata.\n\nNotes\n-----\nThis function is efficient and won't process any pixel data.\n\nThe provided metadata does not include modifications by any filters\n(through ``filter_sequence`` or ``filter_graph``)."
    },
    {
      "name": "metadata",
      "docstring": "Format-specific metadata.\n\nReturns a dictionary filled with metadata that is either stored in the\ncontainer, the video stream, or the frame's side-data.\n\nParameters\n----------\nindex : int\n    If ... (Ellipsis, default) return global metadata (the metadata\n    stored in the container and video stream). If not ..., return the\n    side data stored in the frame at the given index.\nexclude_applied : bool\n    Currently, this parameter has no effect. It exists for compliance with\n    the ImageIO v3 API.\nconstant_framerate : bool\n    If True assume the video's framerate is constant. This allows for\n    faster seeking inside the file. If False, the video is reset before\n    each read and searched from the beginning. If None (default), this\n    value will be read from the container format.\n\nReturns\n-------\nmetadata : dict\n    A dictionary filled with format-specific metadata fields and their\n    values."
    },
    {
      "name": "close",
      "docstring": "Close the Video."
    },
    {
      "name": "init_video_stream",
      "docstring": "Initialize a new video stream.\n\nThis function adds a new video stream to the ImageResource using the\nselected encoder (codec), framerate, and colorspace.\n\nParameters\n----------\ncodec : str\n    The codec to use, e.g. ``\"h264\"`` or ``\"vp9\"``.\nfps : float\n    The desired framerate of the video stream (frames per second).\npixel_format : str\n    The pixel format to use while encoding frames. If None (default) use\n    the codec's default.\nmax_keyframe_interval : int\n    The maximum distance between two intra frames (I-frames). Also known\n    as GOP size. If unspecified use the codec's default. Note that not\n    every I-frame is a keyframe; see the notes for details.\nforce_keyframes : bool\n    If True, limit inter frames dependency to frames within the current\n    keyframe interval (GOP), i.e., force every I-frame to be a keyframe.\n    If unspecified, use the codec's default.\n\nNotes\n-----\nYou can usually leave ``max_keyframe_interval`` and ``force_keyframes``\nat their default values, unless you try to generate seek-optimized video\nor have a similar specialist use-case. In this case, ``force_keyframes``\ncontrols the ability to seek to _every_ I-frame, and\n``max_keyframe_interval`` controls how close to a random frame you can\nseek. Low values allow more fine-grained seek at the expense of\nfile-size (and thus I/O performance)."
    },
    {
      "name": "write_frame",
      "docstring": "Add a frame to the video stream.\n\nThis function appends a new frame to the video. It assumes that the\nstream previously has been initialized. I.e., ``init_video_stream`` has\nto be called before calling this function for the write to succeed.\n\nParameters\n----------\nframe : np.ndarray\n    The image to be appended/written to the video stream.\npixel_format : str\n    The colorspace (pixel format) of the incoming frame.\n\nNotes\n-----\nFrames may be held in a buffer, e.g., by the filter pipeline used during\nwriting or by FFMPEG to batch them prior to encoding. Make sure to\n``.close()`` the plugin or to use a context manager to ensure that all\nframes are written to the ImageResource."
    },
    {
      "name": "set_video_filter",
      "docstring": "Set the filter(s) to use.\n\nThis function creates a new FFMPEG filter graph to use when reading or\nwriting video. In the case of reading, frames are passed through the\nfilter graph before begin returned and, in case of writing, frames are\npassed through the filter before being written to the video.\n\nParameters\n----------\nfilter_sequence : List[str, str, dict]\n    If not None, apply the given sequence of FFmpeg filters to each\n    ndimage. Check the (module-level) plugin docs for details and\n    examples.\nfilter_graph : (dict, List)\n    If not None, apply the given graph of FFmpeg filters to each\n    ndimage. The graph is given as a tuple of two dicts. The first dict\n    contains a (named) set of nodes, and the second dict contains a set\n    of edges between nodes of the previous dict. Check the\n    (module-level) plugin docs for details and examples.\n\nNotes\n-----\nChanging a filter graph with lag during reading or writing will\ncurrently cause frames in the filter queue to be lost."
    },
    {
      "name": "container_metadata",
      "docstring": "Container-specific metadata.\n\nA dictionary containing metadata stored at the container level."
    },
    {
      "name": "video_stream_metadata",
      "docstring": "Stream-specific metadata.\n\nA dictionary containing metadata stored at the stream level."
    },
    {
      "name": "video_filter",
      "docstring": null
    }
  ],
  "innovations": []
}