# Multi-Node Deployment Configuration for Negative Space Imaging Project
# Configuration for distributed cluster deployment across multiple nodes

version: "2.0"
timestamp: "2025-08-08"

# Cluster Configuration
cluster:
  name: "negative-space-cluster"
  environment: "production"  # Options: development, staging, production
  orchestration: "kubernetes"  # Options: kubernetes, docker-swarm, custom
  domain: "negative-space-imaging.com"
  default_replicas: 3
  ha_enabled: true
  auto_scaling: true

# Node Definitions
nodes:
  master:
    count: 3  # For high availability
    role: "control-plane"
    min_resources:
      cpu: 8
      memory: "32Gi"
      storage: "500Gi"
    recommended_resources:
      cpu: 16
      memory: "64Gi"
      storage: "1Ti"
    labels:
      type: "master"
      tier: "control"
    taints:
      - key: "node-role.kubernetes.io/master"
        effect: "NoSchedule"

  compute:
    count: 5
    role: "worker"
    min_resources:
      cpu: 16
      memory: "64Gi"
      storage: "1Ti"
    recommended_resources:
      cpu: 32
      memory: "128Gi"
      storage: "2Ti"
    labels:
      type: "compute"
      tier: "processing"
      gpu: "enabled"
    gpu:
      required: true
      type: "nvidia-tesla-a100"
      count: 4
      memory: "40Gi"
      cuda_version: "11.8"

  storage:
    count: 3
    role: "worker"
    min_resources:
      cpu: 8
      memory: "32Gi"
      storage: "10Ti"
    recommended_resources:
      cpu: 16
      memory: "64Gi"
      storage: "20Ti"
    labels:
      type: "storage"
      tier: "data"
    storage_config:
      type: "distributed-fs"  # Options: distributed-fs, object-storage, block-storage
      filesystem: "ceph"
      replication_factor: 3
      performance_tier: true

  edge:
    count: 2
    role: "worker"
    min_resources:
      cpu: 8
      memory: "16Gi"
      storage: "500Gi"
    recommended_resources:
      cpu: 16
      memory: "32Gi"
      storage: "1Ti"
    labels:
      type: "edge"
      tier: "frontend"

# Networking Configuration
networking:
  topology: "mesh"
  internal_subnet: "10.0.0.0/16"
  service_subnet: "10.1.0.0/16"
  pod_subnet: "10.2.0.0/16"
  node_port_range: "30000-32767"
  load_balancing:
    type: "external"  # Options: external, internal
    provider: "cloud"  # Options: cloud, metalLB, nginx
    public_endpoints:
      - name: "api"
        port: 443
        protocol: "HTTPS"
        target_port: 8000
      - name: "visualization"
        port: 8080
        protocol: "HTTP"
        target_port: 8080
  security:
    network_policy_enabled: true
    encryption_in_transit: true
    ingress_protection:
      ddos_protection: true
      waf_enabled: true

# Service Configuration
services:
  api:
    replicas: 3
    image: "negative-space/api:latest"
    node_selector:
      tier: "processing"
    resources:
      limits:
        cpu: "4"
        memory: "8Gi"
      requests:
        cpu: "2"
        memory: "4Gi"
    environment_variables:
      - name: "NODE_ROLE"
        value: "api"
      - name: "LOG_LEVEL"
        value: "info"
    health_check:
      path: "/api/health"
      port: 8000
      initial_delay_seconds: 30
      period_seconds: 10

  processing_engine:
    replicas: 5
    image: "negative-space/processing:latest"
    node_selector:
      tier: "processing"
      gpu: "enabled"
    resources:
      limits:
        cpu: "16"
        memory: "64Gi"
        nvidia.com/gpu: 2
      requests:
        cpu: "8"
        memory: "32Gi"
        nvidia.com/gpu: 1
    environment_variables:
      - name: "NODE_ROLE"
        value: "processing"
      - name: "GPU_ENABLED"
        value: "true"
      - name: "DISTRIBUTED_MODE"
        value: "true"
    health_check:
      path: "/processing/health"
      port: 8001
      initial_delay_seconds: 60
      period_seconds: 15

  visualization:
    replicas: 2
    image: "negative-space/visualization:latest"
    node_selector:
      tier: "frontend"
    resources:
      limits:
        cpu: "8"
        memory: "16Gi"
        nvidia.com/gpu: 1
      requests:
        cpu: "4"
        memory: "8Gi"
    environment_variables:
      - name: "NODE_ROLE"
        value: "visualization"
      - name: "RENDER_MODE"
        value: "3d"
    health_check:
      path: "/visualization/health"
      port: 8080
      initial_delay_seconds: 45
      period_seconds: 15

  database:
    replicas: 3
    image: "negative-space/database:latest"
    node_selector:
      tier: "data"
    resources:
      limits:
        cpu: "8"
        memory: "32Gi"
      requests:
        cpu: "4"
        memory: "16Gi"
    volume_mounts:
      - name: "db-data"
        mount_path: "/var/lib/data"
        size: "5Ti"
    environment_variables:
      - name: "NODE_ROLE"
        value: "database"
      - name: "REPLICATION_FACTOR"
        value: "3"
    health_check:
      path: "/db/health"
      port: 5432
      initial_delay_seconds: 90
      period_seconds: 20

  security:
    replicas: 2
    image: "negative-space/security:latest"
    node_selector:
      tier: "control"
    resources:
      limits:
        cpu: "4"
        memory: "8Gi"
      requests:
        cpu: "2"
        memory: "4Gi"
    environment_variables:
      - name: "NODE_ROLE"
        value: "security"
      - name: "SECURITY_LEVEL"
        value: "enhanced"
    health_check:
      path: "/security/health"
      port: 8002
      initial_delay_seconds: 30
      period_seconds: 10

# Distributed Processing Configuration
distributed_processing:
  orchestrator: "ray"  # Options: ray, dask, spark
  head_node:
    node_selector:
      tier: "processing"
    resources:
      limits:
        cpu: "16"
        memory: "64Gi"
      requests:
        cpu: "8"
        memory: "32Gi"
  worker_nodes:
    count: "auto"  # Will use all available compute nodes
    node_selector:
      tier: "processing"
    resources:
      limits:
        cpu: "28"
        memory: "120Gi"
        nvidia.com/gpu: 3
      requests:
        cpu: "24"
        memory: "96Gi"
        nvidia.com/gpu: 2
  configuration:
    object_store_memory: "50Gi"
    redis_max_memory: "10Gi"
    scheduler_refresh_interval_ms: 100
    raylet_heartbeat_timeout_ms: 10000

# Storage Configuration
storage:
  persistence:
    enabled: true
    storage_class: "ceph-block"
  raw_data:
    storage_class: "ceph-block"
    size: "10Ti"
    replication_factor: 3
    backup_enabled: true
  processed_data:
    storage_class: "ceph-block"
    size: "5Ti"
    replication_factor: 3
    backup_enabled: true
  model_storage:
    storage_class: "ceph-block"
    size: "1Ti"
    replication_factor: 3
    backup_enabled: true

# Monitoring and Logging
monitoring:
  prometheus:
    enabled: true
    retention_days: 30
    scrape_interval: "15s"
    node_selector:
      tier: "control"
  grafana:
    enabled: true
    dashboards:
      - "node-metrics"
      - "application-metrics"
      - "processing-performance"
      - "visualization-metrics"
      - "security-dashboard"
    node_selector:
      tier: "control"
  alerting:
    enabled: true
    slack_integration: true
    email_integration: true
    pagerduty_integration: true
    rules:
      - name: "node_down"
        severity: "critical"
        description: "Node is down or unreachable"
      - name: "high_memory_usage"
        severity: "warning"
        description: "Memory usage above 85%"
      - name: "gpu_utilization_low"
        severity: "warning"
        description: "GPU utilization below 30%"
      - name: "processing_latency_high"
        severity: "critical"
        description: "Processing latency above threshold"

logging:
  elasticsearch:
    enabled: true
    retention_days: 30
    node_selector:
      tier: "data"
  kibana:
    enabled: true
    node_selector:
      tier: "control"
  fluentd:
    enabled: true
    buffer_size: "256Mi"

# Backup and Disaster Recovery
backup:
  schedule: "0 2 * * *"  # Daily at 2AM
  retention_days: 30
  storage:
    type: "object-storage"
    provider: "aws-s3"
    bucket: "negative-space-backups"
    region: "us-west-2"
  critical_components:
    - "database"
    - "model-storage"
    - "configuration"

disaster_recovery:
  strategy: "active-passive"  # Options: active-passive, active-active
  recovery_point_objective_minutes: 15
  recovery_time_objective_minutes: 60
  secondary_region: "us-east-1"
  automated_failover: true
  failover_tests:
    enabled: true
    schedule: "0 0 * * 0"  # Weekly on Sunday

# Deployment Strategy
deployment:
  strategy: "rolling-update"  # Options: rolling-update, blue-green, canary
  max_surge: "25%"
  max_unavailable: "25%"
  rolling_update:
    max_surge: 1
    max_unavailable: 0
  canary:
    steps:
      - weight: 20
        pause_time_minutes: 15
      - weight: 50
        pause_time_minutes: 15
      - weight: 100
        pause_time_minutes: 0

# Security Configuration
security_settings:
  network_policies:
    default_deny: true
    allowed_namespaces:
      - "kube-system"
      - "monitoring"
      - "logging"
  pod_security_policies:
    privileged: false
    allow_privilege_escalation: false
    read_only_root_filesystem: true
  rbac:
    strict_mode: true
  secrets:
    encryption_at_rest: true
    external_secrets: true
    vault_integration: true

# Auto-scaling Configuration
auto_scaling:
  cluster_autoscaler:
    enabled: true
    min_nodes: 10
    max_nodes: 50
    scale_down_unneeded_time: "10m"
    scale_down_utilization_threshold: "0.5"
  horizontal_pod_autoscaler:
    enabled: true
    min_replicas: 1
    max_replicas: 10
    target_cpu_utilization_percentage: 75
    target_memory_utilization_percentage: 75
  vertical_pod_autoscaler:
    enabled: true
    update_mode: "Auto"
